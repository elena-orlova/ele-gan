{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(conflict_handler='resolve')\n",
    "parser.add = parser.add_argument\n",
    "parser.add('--gpu_id', type=int, default=0)\n",
    "parser.add('--image_size', type=int, default=16)\n",
    "parser.add('--num_epoch', type=int, default=200)\n",
    "parser.add('--val_epoch_freq', type=int, default=5)\n",
    "parser.add('--batch_size', type=int, default=64)\n",
    "parser.add('--num_channels', type=int, default=256)\n",
    "parser.add('--data_type', type=str, default='norm')\n",
    "parser.add('--target_type', type=str, default='none')\n",
    "parser.add('--train_path', type=str, default='data/ecalNT_50K_e_10_100.npz')\n",
    "parser.add('--val_path', type=str, default='data/ecalNT_10K_e_10_100.npz')\n",
    "parser.add('--regressor_path', type=str, default='checkpoints/classifier/regressor_none')\n",
    "\n",
    "\n",
    "opt, _ = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ezakharov/miniconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '%d' % opt.gpu_id\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from models.dcgan import Generator, Discriminator\n",
    "from src.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from src.stats import Stats\n",
    "from src.utils import calc_gradient_penalty\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data, load stats tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(opt)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "    opt.batch_size, True, num_workers=4, drop_last=True)\n",
    "val_dataset = Dataset(opt, train_dataset)\n",
    "val_loader = DataLoader(val_dataset, \n",
    "    opt.batch_size, True, num_workers=4, drop_last=True)\n",
    "stats = Stats(opt)\n",
    "stats.calc_stats(val_dataset.data, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set networks, criterions and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = Generator(opt.num_channels, opt.image_size).cuda()\n",
    "dis = Discriminator(opt.num_channels, opt.image_size).cuda()\n",
    "optim_g = Adam(gen.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "optim_d = Adam(dis.parameters(), lr=1e-4, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "781it [00:13, 57.79it/s]\n",
      "781it [00:13, 57.89it/s]\n",
      "781it [00:13, 58.20it/s]\n",
      "781it [00:13, 58.35it/s]\n",
      "60it [00:01, 57.73it/s]"
     ]
    }
   ],
   "source": [
    "for e in range(opt.num_epoch):\n",
    "    # train for 1 epoch\n",
    "    gen.train()\n",
    "    dis.train()\n",
    "    train_wgan_loss = 0\n",
    "    for i, (input_data, input_target) in tqdm(enumerate(train_loader, 0)):\n",
    "        # train dis\n",
    "        for p in dis.parameters():\n",
    "            p.requires_grad = True\n",
    "        optim_d.zero_grad()\n",
    "        input = Variable(torch.randn(opt.batch_size, opt.num_channels)).cuda()\n",
    "        fake = gen(input)\n",
    "        real = Variable(input_data).cuda()\n",
    "        pred_real = dis(real).mean()\n",
    "        pred_fake = dis(fake.detach()).mean()\n",
    "        loss_dis = pred_fake - pred_real + calc_gradient_penalty(dis, real.data, fake.data)\n",
    "        loss_dis.backward()\n",
    "        optim_d.step()\n",
    "        train_wgan_loss += (pred_fake - pred_real).data[0]\n",
    "        # train gen\n",
    "        if i % 5 != 4:\n",
    "            continue\n",
    "        for p in dis.parameters():\n",
    "            p.requires_grad = False\n",
    "        optim_g.zero_grad()\n",
    "        pred_fake = dis(fake).mean()\n",
    "        loss_gen = -pred_fake\n",
    "        loss_gen.backward()\n",
    "        optim_g.step()\n",
    "    train_wgan_loss /= (i+1)\n",
    "    # validate after val_epoch_freq epoch\n",
    "    if e % opt.val_epoch_freq != opt.val_epoch_freq-1:\n",
    "        continue\n",
    "    gen.eval()\n",
    "    dis.eval()\n",
    "    stats.train_loss.append(train_wgan_loss)\n",
    "    val_wgan_loss = 0\n",
    "    val_data_fake = []\n",
    "    for i, (input_data, input_target) in tqdm(enumerate(val_loader, 0)):\n",
    "        input = Variable(torch.randn(opt.batch_size, opt.num_channels), volatile=True).cuda()\n",
    "        fake = gen(input)\n",
    "        val_data_fake.append(fake.data.cpu().numpy()[:, 0])\n",
    "        real = Variable(input_data).cuda()\n",
    "        pred_real = dis(real).mean()\n",
    "        pred_fake = dis(fake).mean()\n",
    "        val_wgan_loss += (pred_fake - pred_real).data[0]\n",
    "    val_wgan_loss /= (i+1)\n",
    "    stats.val_loss.append(val_wgan_loss)\n",
    "    val_data_fake = np.concatenate(val_data_fake, 0)\n",
    "    val_data_fake = val_dataset.get_output(val_data_fake, opt.data_type)\n",
    "    clear_output()\n",
    "    stats.calc_stats(val_data_fake)\n",
    "    f = stats.get_plot()\n",
    "    f.savefig('checkpoints/generator/figures/%d.pdf' % (e+1))\n",
    "    torch.save(gen.state_dict(), 'checkpoints/generator/weights/gen_%d.pkl' % (e+1))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
